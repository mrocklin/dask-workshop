{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\\\">\n",
    "\n",
    "\n",
    "# Schedulers and Efficiency\n",
    "\n",
    "In the previous two notebooks we used Dask.delayed and Dask.dataframe to create computations.  By default, these ran on a local thread pool on our personal machines.  Often, this is sufficient, especially when you are bound by NumPy and Pandas routines which release the GIL and when you are using powerful workstation computers with many cores.\n",
    "\n",
    "However sometimes you may want to execute your code in processes (for Pure Python code that holds onto the GIL), in a single thread (for profiling and debugging) or across a cluster (for larger computations).\n",
    "\n",
    "In this section we first talk about changing schedulers.  Then we use the dask.distributed scheduler in more depth.  Finally we redo some of our dataframe computations, but with an eye to efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous exercise\n",
    "\n",
    "One solution to the previous exercise follows below.  We run this same computation using three single-machine schedulers:\n",
    "\n",
    "1.  `dask.threaded.get         # uses a local threadpool`\n",
    "2.  `dask.multiprocessing.get  # uses a local process pool`\n",
    "3.  `dask.async.get_sync       # uses only the main thread (useful for debugging)`\n",
    "\n",
    "In each case we change the scheduler by providing a `get=` keyword argument like the following:\n",
    "\n",
    "```python\n",
    "total.compute(get=dask.multiprocessing.get)\n",
    "# or \n",
    "dask.compute(a, b, get=dask.multiprocessing.get)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask \n",
    "import dask.multiprocessing\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df = dd.read_csv(os.path.join('data', 'stocks', 'GOOG', '*.csv'), parse_dates=['timestamp'])\n",
    "\n",
    "high = df.groupby(df.timestamp.dt.round('1d')).high.max()\n",
    "low = df.groupby(df.timestamp.dt.round('1d')).low.min()\n",
    "spread = high - low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time _ = spread.compute()  # this uses threads by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time _ = spread.compute(get=dask.multiprocessing.get)  # this uses processes by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time _ = spread.compute(get=dask.async.get_sync)  # This uses a single thread by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling\n",
    "\n",
    "*You should skip this section if you are running low on time*.\n",
    "\n",
    "The synchronous scheduler is particularly valuable for debugging and profiling.  \n",
    "\n",
    "For example, the IPython `%%prun` magic gives us profiling information about which functions take up the most time in our computation.  Try this magic on the computation above with each of the schedulers.  How informative is this magic when running parallel code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%prun _ = spread.compute(get=dask.threaded.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%prun _ = spread.compute(get=dask.multiprocessing.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%prun _ = spread.compute(get=dask.async.get_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:  In what cases would you want to use one scheduler over another?\n",
    "\n",
    "http://dask.pydata.org/en/latest/scheduler-choice.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Scheduler\n",
    "\n",
    "The dask.distributed system is composed of a single centralized scheduler and several worker processes.  We can either set these up manually as command line processes or have Dask set them up for us from the notebook.  \n",
    "\n",
    "\n",
    "#### Automatic\n",
    "\n",
    "Starting a single scheduler and worker on the local machine is the common case.  Sometimes using the command line can be annoying.  Dask will set everything up for you if you start a client with no arguments\n",
    "\n",
    "```python\n",
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "```\n",
    "\n",
    "If you choose this approach then there is no need to set up a `dask-scheduler` or `dask-worker` process as described below.\n",
    "\n",
    "\n",
    "#### Manual\n",
    "\n",
    "It is good to know how to set things up manually in case you want to try out Dask on a small cluster of your own.  However, if you are unfamiliar with the command line you can safely skip this section and go down to the Automatic section below. We run the `dask-scheduler` process on one machine.\n",
    "\n",
    "    $ dask-scheduler\n",
    "    distributed.scheduler - INFO -   Scheduler at:  tcp://127.0.0.1:8786\n",
    "    distributed.bokeh.application - INFO - Web UI: http://127.0.0.1:8787/status/\n",
    "\n",
    "The scheduler reports that it is running at 127.0.0.1 on port 8786.  The address 127.0.0.1 is another name for \"localhost\" or \"this machine\".  We will need to give this address to the workers and client so you might want to copy it now.\n",
    "\n",
    "We now run the `dask-worker` process on every machine that we want to use for computation.  For right now this is probably just once on our laptop, but in production this may be on many different machines:\n",
    "\n",
    "    $ dask-worker 127.0.0.1:8786\n",
    "    distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45011\n",
    "    distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:8786\n",
    "\n",
    "*Note*: for this tutorial we want to start the dask-worker process in the `dask-workshop/` directory.  This will ensure that the workers Python processes have access to the same data that our notebook process does.\n",
    "\n",
    "    $ cd dask-workshop/  # navigate to whereever you have started your notebooks\n",
    "    ~/dask-workshop/ $ dask-worker 127.0.0.1:8786\n",
    "\n",
    "*Note*: By default the dask-worker command line tool starts a single process with a thread pool with as many threads as you have cores on your computer.  If you are doing mostly GIL-released computations (numpy, pandas, scikit-learn) then this is the right choice.  However if you are doing mostly GIL-bound comptutations (Python code, pandas with text, parsing) then you will want to start the workers with multiple processes and one thread per process\n",
    "\n",
    "    $ dask-worker 127.0.0.1:8786 --nprocs 8 --nthreads 4\n",
    "    \n",
    "You can see more options by asking for help\n",
    "\n",
    "    $ dask-worker --help\n",
    "    \n",
    "When the scheduler and workers are running you can connect to them using a Dask `Client`, giving it the same address of the scheduler that you gave to the worker.\n",
    "\n",
    "```python\n",
    "from dask.distributed import Client\n",
    "client = Client('127.0.0.1:8786')\n",
    "```\n",
    "### More information\n",
    "\n",
    "You can find more information at the following documentation pages:\n",
    "\n",
    "- [Quickstart](http://distributed.readthedocs.io/en/latest/quickstart.html)\n",
    "- [Setup Network](http://distributed.readthedocs.io/en/latest/setup.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise: Start a client \n",
    "#           that points to `'127.0.0.1:8786'` if you started a `dask-scheduler` manually \n",
    "#           or with no arguments (like `Client()`) if you want Dask to set things up for you.\n",
    "\n",
    "from dask.distributed import Client\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostics\n",
    "\n",
    "One of the main advantages of using the distributed scheduler is the diagnostics dashboards that should be hosted live at http://localhost:8787/status .  Visit that link and then run the computation again.\n",
    "\n",
    "You may want to arrange your notebook and this webpage side-by-side on your screen so that you can see both at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time _ = spread.compute(get=client.get)  # This uses our \"distributed\" cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client takes over by default\n",
    "\n",
    "Actually, we didn't need to add `get=client.get`.  The distributed scheduler takes over as the default scheduler by default when the Client is instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time _ = spread.compute()  # This used to use threads by default, now it uses dask.distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New API\n",
    "\n",
    "The distributed scheduler is more sophisticated than the single machine schedulers.  It comes with more functions to manage data, computing in the background, and more.  The distributed scheduler also has entirely separate documentation\n",
    "\n",
    "-  http://distributed.readthedocs.io/en/latest/\n",
    "-  http://distributed.readthedocs.io/en/latest/api.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficiency\n",
    "\n",
    "In this section we combine the distributed scheduler with our dask.dataframe exercises to learn about how to make our computations more efficient.  We will cover the following topics:\n",
    "\n",
    "1.  Persist common intermediate results in memory with `persist`\n",
    "2.  Reduce per-task overhead by repartitioning our datafarmes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist data in distributed memory\n",
    "\n",
    "Every time we run an operation like `df.high.max().compute()` we read through our dataset from disk.  This can be slow, especially because we're reading data from CSV.  We usually have two options to make this faster:\n",
    "\n",
    "1.  Persist relevant data in memory, either on our computer or on a cluster\n",
    "2.  Use a faster on-disk format, like HDF5 or Parquet\n",
    "\n",
    "In this section we persist our data in memory.  On a single machine this is often done by doing a bit of pre-processing and data reduction with dask dataframe and then `compute`-ing to a Pandas dataframe and using Pandas in the future.  \n",
    "\n",
    "```python\n",
    "df = dd.read_csv(...)\n",
    "df = df[df.account == 1234]  # filter down to smaller dataset\n",
    "pdf = df.compute()  # convert to pandas\n",
    "pdf ... # continue with familiar Pandas workflows\n",
    "```\n",
    "\n",
    "However on a distributed cluster when even our cleaned data is too large we still can't use Pandas.  In this case we ask Dask to persist data in memory with the `dask.persist` function.  This is what we'll do today.  This will help us to understand when data is lazy and when it is computing.\n",
    "\n",
    "You can trigger computations using the persist method:\n",
    "\n",
    "    x = x.persist()\n",
    "\n",
    "or the dask.persist function for multiple inputs:\n",
    "\n",
    "    x, y = dask.persist(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Persist the dataframe into memory.\n",
    "\n",
    "-  After it has persisted how long does it take to compute `df.high.max()`?\n",
    "-  Looking at the plots in the [diagnostic web page](http://localhost:8787/status), what is taking up most of the time?  (You can over over rectangles to see what function they represent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time df.high.max().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = # TODO: persist dataframe in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Copy-paste the Daily High-Low Spread plot from above.  How much faster is it?  What is taking all of the time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitions\n",
    "\n",
    "One Dask.dataframe is composed of several Pandas dataframes.  The organization of these dataframes can significantly impact performance.  In this section we discuss two common factors that commonly impact performance:\n",
    "\n",
    "1.  The number of Pandas dataframes can affect overhead.  If the dataframes are too small then Dask might spend more time deciding what to do than Pandas spends actually doing it.  Ideally computations should take 100's of milliseconds.\n",
    "\n",
    "2.  If we know how the dataframes are sorted then certain operations become much faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of partitions and repartitioning\n",
    "\n",
    "When we read in our data from CSV files we got one Pandas dataframe for each day.  Look at the metadata below to determine how many partitions we have.  Each \"partition\" is a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Roughly how large is each partition?\n",
    "\n",
    "There are a few ways to answer this:\n",
    "\n",
    "1.  Look at the diagnostic dashboard to see how much memory is being used.  Divide this by the number of partitions.\n",
    "2.  Use the [.map_partitions()](http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.map_partitions) method along with the `pandas.DataFrame.memory_usage().sum()` function to determine how many bytes each partition consumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our partitions in our dataframe are somewhat small.  This is because the data for every day isn't very large.  This means that Dask may spend more time scheduling computations than Pandas actually spends running them.  We would like to partition our data so that our individual Pandas dataframes are roughly ~100MB each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the number of partitions with repartition\n",
    "\n",
    "We can bring partitions together with the [.repartition](http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.repartition) method.  Be sure to persist the dataframe afterwards so that we don't do the repartition step over and over again.  About 20 partitions is probably a good number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare timings\n",
    "\n",
    "Use the diagnostic dashboard and the `%time` magic to compare the speed of some operations that we did above.  How have things improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorted Index column\n",
    "\n",
    "*This section doesn't have any exercises.  Just follow along.*\n",
    "\n",
    "Many dataframe operations like loc-indexing, groupby-apply, and joins are *much* faster on a sorted index.  For example, if we want to get data for a particular day of data it *really* helps to know where that day is, otherwise we need to search over all of our data.\n",
    "\n",
    "The Pandas model gives us a sorted index column.  Dask.dataframe copies this model, and it remembers the min and max values of every partition's index.\n",
    "\n",
    "By default, our data doesn't have an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we search for a particular day it takes a while because it has to pass through all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time df[df.timestamp.dt.round('1d') == '2015-05-05'].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However if we set the timestamp column as the index then this operation can be much much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df = df.set_index('timestamp')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time df.loc['2015-05-05'].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally this lets us do traditional Pandas timeseries functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "(df.close\n",
    "   .resample('1d')\n",
    "   .mean()\n",
    "   .fillna(method='ffill')\n",
    "   .compute()\n",
    "   .plot(figsize=(10, 5)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
