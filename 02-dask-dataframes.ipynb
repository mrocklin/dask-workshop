{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\\\">\n",
    "\n",
    "\n",
    "# Dask DataFrames\n",
    "\n",
    "We finished the last section by building a parallel dataframe computation over a directory of CSV files using Dask.delayed.  In this section we use Dask.dataframes to build computations for us in the common case of tabular computations.  Dask dataframes look and feel like Pandas dataframes but they run on the same infrastructure that powers dask.delayed (indeed many dask.dataframe functions are built using dask.delayed).\n",
    "\n",
    "In this notebook we use the same stock data as in notebook 01, but now rather than write for loops we let dask.dataframe construct our computations for us.  The `dask.dataframe.read_csv` function can take a globstring like `\"data/stocks/GOOG/*.csv\"` and build parallel computations on all of our data at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "%matplotlib inline\n",
    "\n",
    "df = dd.read_csv(os.path.join('data', 'stocks', 'GOOG', '*.csv'), parse_dates=['timestamp'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the maximum of the `high` column.  With Dask.delayed we could create this computation as follows:\n",
    "\n",
    "```python\n",
    "maxes = []\n",
    "for fn in filenames:\n",
    "    df = dask.delayed(pd.read_csv)(fn)\n",
    "    maxes.append(df.high.max())\n",
    "    \n",
    "final_max = dask.delayed(max)(maxes)\n",
    "final_max.compute()\n",
    "```\n",
    "\n",
    "Now we just use the normal Pandas syntax as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time df.high.max().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This writes the dask.delayed computation for us and then runs it.  \n",
    "\n",
    "Some things to note:\n",
    "\n",
    "1.  As with dask.delayed, we need to call `.compute()` when we're done.  Up until this point everything is lazy.\n",
    "2.  Dask will delete intermediate results (like the full pandas dataframe for each file) as soon as possible.\n",
    "    -  This lets us handle datasets that are larger than memory\n",
    "    -  This means that repeated computations will have to load all of the data in each time (run the code above again, is it faster or slower than you would expect?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "In this section we do a few trivial dask dataframe computations.  If you are comfortable with Pandas then these should be familiar.  You will have to think about when to call `compute`.\n",
    "\n",
    "-  How many rows are in our dataset?\n",
    "-  Read the first few rows of the timestamp column \n",
    "-  Use the `.dt.round` method to round the timestamp column to days\n",
    "-  Get the high value for each day by grouping by the result from above and computing the maximum of the high column per group\n",
    "-  Compute the daily high-low spread.  This is exactly the result from the final exercise in the dask.delayed notebook\n",
    "-  Plot the resuling Pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do the same computation we did in the previous section, but now with a few lines of Pandas-ish code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the first few rows of the timestamp column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the `.dt.round('1d')` method to round the timestamp column to days\n",
    "# Show the first few rows to make sure it works well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the high value for each day by grouping by the result from above \n",
    "# and computing the maximum of the high column per group\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the daily high-low spread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "_.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load solutions/03-dataframe-spread.py"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
