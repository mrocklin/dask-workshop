{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\\\">\n",
    "\n",
    "\n",
    "# Delayed DataFrames\n",
    "\n",
    "In two of the previous notebooks we saw two different methods to build parallel computations with Dask\n",
    "\n",
    "1.  Use Dask.delayed to wrap custom code\n",
    "2.  Use Dask.dataframe to handle large dataframes \n",
    "\n",
    "Most non-trivial problems require both of these methods.  We often deal with tabular data, but messy situations arise where we need to handle things manually.  For example our data might be in a messy form that requires special attention, or we may want to execute an algorithm that is not implemented in dask.dataframe.\n",
    "\n",
    "In this notebook we use Dask.delayed to load some custom data and then convert these delayed values into a Dask dataframe.  This shows us how to use both together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to convert between dask dataframes and dask delayed\n",
    "\n",
    "If you have a dask.dataframe, you can construct a list of delayed objects pointing to each of the dataframe's partitions\n",
    "\n",
    "```python\n",
    ">>> df.to_delayed()\n",
    "[...]\n",
    "```\n",
    "\n",
    "If you have a list of delayed values, each of which create a single Pandas dataframe, you can construct a Dask.dataframe.\n",
    "\n",
    "```python\n",
    ">>> parts = [delayed(read_pandas_dataframe)(arg) for arg in args]\n",
    ">>> df = dd.from_delayed(parts)\n",
    "```\n",
    "\n",
    "Consider the following example ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def f(n):\n",
    "    \"\"\" This returns a simple Pandas DataFrame with n rows \"\"\"\n",
    "    return pd.DataFrame({'x': [i for i in range(n)],\n",
    "                         'y': [i ** 2 for i in range(n)]})\n",
    "\n",
    "f(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "lazy_dataframes = [dask.delayed(f)(n) for n in [1, 3, 5, 7]]\n",
    "lazy_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "df = dd.from_delayed(lazy_dataframes)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "\n",
    "You can ignore this section.  It moves around our data in a way that makes it efficient, but also somewhat more complex.  We'll learn more about what we have to do with this data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "filenames = sorted(glob.glob(os.path.join('data', 'stocks', '*', '*.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirname = os.path.join('data', 'messy')\n",
    "if os.path.exists(dirname):\n",
    "    import shutil\n",
    "    shutil.rmtree(dirname)\n",
    "    \n",
    "os.mkdir(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import feather\n",
    "\n",
    "def convert(fn):\n",
    "    data, _, symbol, date = fn.split(os.sep)\n",
    "    date = date.split('.')[0]\n",
    "    df = pd.read_csv(fn, parse_dates=['timestamp'])\n",
    "    df['timestamp'] = ((df.timestamp - df.timestamp.dt.floor('1d')).astype(int)/ 1e9).astype('int32')\n",
    "    new_fn = os.path.join(data, 'messy', date, symbol + '.feather')\n",
    "    if not os.path.exists(os.path.dirname(new_fn)):\n",
    "        os.mkdir(os.path.dirname(new_fn))\n",
    "    feather.write_dataframe(df, new_fn)\n",
    "\n",
    "import dask\n",
    "import dask.multiprocessing\n",
    "values = [dask.delayed(convert)(fn) for fn in filenames]\n",
    "\n",
    "dask.compute(values, get=dask.multiprocessing.get);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel access to custom data formats\n",
    "\n",
    "Imagine that you work for a company that cares about financial time series data for many stocks over time.  Last year your company decided to organize data into a special directory structure that puts each day in a separate directory and then each stock in a separate file within this directory.  Your company has chosen to use the new Feather format because it is more efficient than CSV.\n",
    "\n",
    "This makes your data look something like the following:\n",
    "\n",
    "```\n",
    "data/messy/2015-01-01\n",
    "├── AAPL.feather\n",
    "├── GOOG.feather\n",
    "├── MSFT.feather\n",
    "└── YHOO.feather\n",
    "data/messy/2015-01-02\n",
    "├── AAPL.feather\n",
    "├── GOOG.feather\n",
    "├── MSFT.feather\n",
    "└── YHOO.feather\n",
    "data/messy/2015-01-03\n",
    "├── AAPL.feather\n",
    "├── GOOG.feather\n",
    "├── MSFT.feather\n",
    "└── YHOO.feather\n",
    "```\n",
    "\n",
    "Each file contains the high/low/open/close values, along with the seconds within each day.  Lets look at the data for a single day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import feather\n",
    "fn = os.path.join('data', 'messy', '2015-01-02', 'GOOG.feather')\n",
    "df = feather.read_dataframe(fn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data into Pandas\n",
    "\n",
    "Again for efficiency, your company has decided that each feather file includes neither the stock symbol  nor the date, because these are both encoded in the filename.  However, when people want to compare many files at the same time they end up adding this information back in.  \n",
    "\n",
    "Your colleague has written a small routine to load all of the data into a single Pandas DataFrame.  It does the following:\n",
    "\n",
    "1.  Load each dataframe into memory\n",
    "2.  Alter the dataframe to include the symbol name and date in the filename\n",
    "3.  Concatenate all of these Pandas dataframes into a larger Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for dir in sorted(glob.glob(os.path.join('data', 'messy', '*'))):\n",
    "    for fn in sorted(glob.glob(os.path.join(dir, '*'))):\n",
    "        _, _, date, symbol = fn.split(os.path.sep)\n",
    "        symbol = symbol[:-len('.feather')]\n",
    "        date = pd.Timestamp(date)\n",
    "        df = feather.read_dataframe(fn)\n",
    "        df['timestamp'] = df.timestamp.astype('m8[s]') + date\n",
    "        df['symbol'] = symbol\n",
    "        dfs.append(df)\n",
    "        \n",
    "df = pd.concat(dfs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelize \n",
    "\n",
    "This routine works well and has become popular within the company on small datasets.  However your company is now anticipating getting much bigger data in the near future and wants to be able to scale out this process beyond just Pandas.  \n",
    "\n",
    "You have been asked to paralellize your colleague's code so that it can run in parallel and scale out to a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Delayed + Dataframes\n",
    "\n",
    "Build a lazy Dask dataframe from the sequential dataframe munging code we had above.  You will have to use dask.delayed to parallelize/lazify the for-loop code from before and then use `dd.from_delayed` to convert these many lazy Pandas dataframes into a dask.dataframe.\n",
    "\n",
    "*Hint: You may at some point need to rely on [pandas.DataFrame.assign](http://pandas.pydata.org/pandas-docs/version/0.18.1/generated/pandas.DataFrame.assign.html) to avoid mutating a delayed object*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sequential Code\n",
    "\n",
    "dfs = []\n",
    "for dir in sorted(glob.glob(os.path.join('data', 'messy', '*'))):\n",
    "    for fn in sorted(glob.glob(os.path.join(dir, '*'))):\n",
    "        _, _, date, symbol = fn.split(os.path.sep)\n",
    "        symbol = symbol[:-len('.feather')]\n",
    "        date = pd.Timestamp(date)\n",
    "        df = feather.read_dataframe(fn)\n",
    "        df['timestamp'] = df.timestamp.astype('m8[s]') + date\n",
    "        df['symbol'] = symbol\n",
    "        dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Parallel code\n",
    "\n",
    "# TODO: Parallelize the sequential code above using dask.delayed.  \n",
    "# Get back a Dask.dataframe \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load solutions/04-delayed-dataframes.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
